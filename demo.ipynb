{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'state-spaces/mamba-130m'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "model, params = Mamba.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "JAX Devices: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "\n",
    "print(xla_bridge.get_backend().platform)\n",
    "print('JAX Devices:', '\\n'.join([d.device_kind for d in jax.devices()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,\n",
    "            params, \n",
    "            tokenizer,\n",
    "            prompt: str,\n",
    "            n_tokens_to_gen: int = 50,\n",
    "            sample: bool = True,\n",
    "            top_k: int = 40,\n",
    "            rng = jax.random.PRNGKey(177013)):\n",
    "    \n",
    "    # Encode prompt to tokens\n",
    "    input_ids = tokenizer(prompt, return_tensors='jax').input_ids\n",
    "\n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        # Get the logits of the last predicted token\n",
    "        next_token_logits = model.apply(params, input_ids)[:, -1]\n",
    "\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probs = jax.nn.softmax(next_token_logits)\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None:\n",
    "            (values, indices) = jax.lax.top_k(probs, k=top_k)\n",
    "            mask = probs < values[..., -1, None]\n",
    "            probs = probs.at[mask].set(0)\n",
    "            probs = probs / jnp.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "        if sample:\n",
    "            # Sample the next token indices\n",
    "            next_indices = jax.random.categorical(rng, probs, num_samples=1)\n",
    "        else:\n",
    "            # Pick the most likely next token\n",
    "            next_indices = jnp.argmax(probs, axis=-1, keepdims=True)\n",
    "\n",
    "        # Append next token ID to the sequence\n",
    "        input_ids = jnp.concatenate([input_ids, next_indices], axis=1)\n",
    "    \n",
    "    # Decode generated tokens to text\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "\n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, params, tokenizer, 'Mamba is the', sample=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
